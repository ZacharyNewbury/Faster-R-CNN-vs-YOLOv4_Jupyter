{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"detectron2 eval.ipynb","provenance":[],"authorship_tag":"ABX9TyMDMdxNAYSLJzrRJwdkyKwH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"NoeutpV4PfrO","executionInfo":{"status":"ok","timestamp":1618838482366,"user_tz":240,"elapsed":18442,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"e48ba1ba-1801-46c1-c3d4-e71842a6339d"},"source":["#@title connect to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"NripdGFK51h1","executionInfo":{"status":"ok","timestamp":1618838461514,"user_tz":240,"elapsed":730,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"ce01e278-5b6d-4691-de04-0f5664ddb4fe"},"source":["#@title check the gpu before running, should be P100\n","import os\n","os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB\\n'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yg8jF_zlPNDl","executionInfo":{"status":"ok","timestamp":1618838605333,"user_tz":240,"elapsed":128887,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"7cdfb7a2-0783-4cfb-cb88-d070332bbc5e"},"source":["#@title setup detectron2\n","# install dependencies: \n","!pip install pyyaml==5.1\n","!pip install torch==1.7\n","!pip install torchvision==0.8.1\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version\n","import torch\n","assert torch.__version__.startswith(\"1.7\")\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n","exit(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyyaml==5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n","\r\u001b[K     |█▏                              | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 24.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 29.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 17.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 14.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 13.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 13.1MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyyaml\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44074 sha256=a5cf40d944d6ab72da5e9cae4c618b4416be517fec27f06ed7dbce8155afffd9\n","  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n","Successfully built pyyaml\n","Installing collected packages: pyyaml\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-5.1\n","Collecting torch==1.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (1.19.5)\n","Collecting dataclasses\n","  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.16.0)\n","\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n","Installing collected packages: dataclasses, torch\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","Successfully installed dataclasses-0.6 torch-1.7.0\n","Collecting torchvision==0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/39/a9caac0deb027feec2cdd7cc40b2a598256d3f50050c80f349c030f915f2/torchvision-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (12.7MB)\n","\u001b[K     |████████████████████████████████| 12.7MB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1) (1.19.5)\n","Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1) (1.7.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.1) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.1) (0.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.1) (0.16.0)\n","Installing collected packages: torchvision\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","Successfully installed torchvision-0.8.1\n","1.7.0 True\n","gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n","Copyright (C) 2017 Free Software Foundation, Inc.\n","This is free software; see the source for copying conditions.  There is NO\n","warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n","\n","Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n","Collecting detectron2\n","\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/detectron2-0.4%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.0MB)\n","\u001b[K     |████████████████████████████████| 6.0MB 695kB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.2)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n","Collecting yacs>=0.1.6\n","  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.4.1)\n","Collecting omegaconf>=2\n","  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n","Collecting fvcore<0.1.4,>=0.1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/68/2bacb80e13c4084dfc37fec8f17706a1de4c248157561ff33e463399c4f5/fvcore-0.1.3.post20210317.tar.gz (47kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n","\u001b[?25hCollecting iopath>=0.1.2\n","  Downloading https://files.pythonhosted.org/packages/21/d0/22104caed16fa41382702fed959f4a9b088b2f905e7a82e4483180a2ec2a/iopath-0.1.8-py3-none-any.whl\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.41.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (54.2.0)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.22)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot->detectron2) (2.4.7)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs>=0.1.6->detectron2) (5.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.28.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.15.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.36.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.12.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.32.0)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf>=2->detectron2) (3.7.4.3)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (3.10.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n","Building wheels for collected packages: fvcore\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.3.post20210317-cp37-none-any.whl size=58543 sha256=463c421965dbefb850ca31a953e8b7e5f7f028c7c30bea272916729974f79d5b\n","  Stored in directory: /root/.cache/pip/wheels/d2/ee/3a/5c531df777c03d8c67f22c65f97d6f75321087482d05a9b218\n","Successfully built fvcore\n","Installing collected packages: yacs, omegaconf, portalocker, iopath, fvcore, detectron2\n","Successfully installed detectron2-0.4+cu101 fvcore-0.1.3.post20210317 iopath-0.1.8 omegaconf-2.0.6 portalocker-2.3.0 yacs-0.1.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpQZ1Kx5PpIJ","executionInfo":{"status":"ok","timestamp":1618838621893,"user_tz":240,"elapsed":1063,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"566b890a-9d59-41c1-cf00-2539508a665b"},"source":["#@title setup variables and imports\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random, time, ntpath\n","from shutil import copyfile\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","\n","\n","def print_time(log):\n","  current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n","  print(current_time+\": \"+log)\n","\n","\n","\n","#base path for training images\n","thesis_path = \"/content/drive/MyDrive/thesis\"\n","LISA_path = \"/content/drive/MyDrive/thesis/LISA dataset/\"\n","YOLO_path = \"/content/drive/MyDrive/thesis/YOLO/dataset\"\n","COCO_path = \"/content/drive/MyDrive/thesis/COCO\"\n","\n","#@markdown model version, increment each run\n","def getModel(num):\n","  mnum = (str)(num)\n","  return os.path.join(COCO_path,'model'+(str)(mnum))\n","\n","#make directory if dne\n","#os.makedirs(model_path, exist_ok=True)\n","trainer = None\n","\n","#fill things variable for metadata\n","things=[]\n","for line in open(os.path.join(LISA_path, \"categories.txt\"), \"r\").read().split('\\n'):\n","  things.append(line)\n","\n","print_time(\"printing time to test\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["13:23:41: printing time to test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0KN7qeoPutx","executionInfo":{"status":"ok","timestamp":1618838627530,"user_tz":240,"elapsed":994,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"19ac9cf5-a769-4378-b6c0-6df810dc7020"},"source":["#@title register datasets\n","from detectron2.structures import BoxMode \n","d = [\"Train\", \"Valid\", \"Test\"]\n","\n","data = dict()\n","print_time(\"reading data\")\n","for h in d:\n","  with open(os.path.join(LISA_path, \"COCO_Annotations_\"+h+\".json\"), \"r\") as outfile:\n","      data[h.lower()] = json.load(outfile)\n","      outfile.close()\n","\n","for h in d:\n","  for row in data[h.lower()]:\n","    row[\"annotations\"][0][\"bbox_mode\"] = BoxMode.XYXY_ABS\n","\n","train=data[\"train\"]\n","valid=data[\"valid\"]\n","test=data[\"test\"]\n","print_time(\"finished reading data\")\n","\n","#register custom dataset for use\n","from detectron2.data import DatasetCatalog, MetadataCatalog\n","\n","def get_sign_dicts_premade(data):\n","  return data\n","\n","#data - data list in standard format\n","#s - string representing which dataset to use\n","#isCOCO - if its to be registered as a coco dataset\n","def registerDataset(data, s):\n","    DatasetCatalog.register(\"signs_\"+s, lambda : get_sign_dicts_premade(data))\n","    MetadataCatalog.get(\"signs_\"+s).set(thing_classes=things)\n","    print_time(\"registered \"+s+\" as a general dataset\")\n","\n","print_time(\"registering datasets\")\n","registerDataset(train, \"train\")\n","registerDataset(valid, \"valid\")\n","registerDataset(test, \"test\")\n","\n","sign_metadata = MetadataCatalog.get(\"signs_train\")\n","print_time(\"datasets finished registering\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["13:23:46: reading data\n","13:23:46: finished reading data\n","13:23:46: registering datasets\n","13:23:46: registered train as a general dataset\n","13:23:46: registered valid as a general dataset\n","13:23:46: registered test as a general dataset\n","13:23:46: datasets finished registering\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WL_4Eo6AQTno","executionInfo":{"status":"ok","timestamp":1618838625459,"user_tz":240,"elapsed":1774,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"646a2769-b63a-421c-ce7c-6254030776fc"},"source":["#@title clear dataset catalog if error registering\n","DatasetCatalog.clear()\n","MetadataCatalog.clear()\n","print_time(\"datasets cleared\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["13:23:44: datasets cleared\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LYHE0AqWQYos"},"source":["#@title create custom trainer class\n","from detectron2.engine import DefaultTrainer\n","\n","#custom trainer based on example given from facebook https://github.com/facebookresearch/detectron2/blob/master/tools/train_net.py\n","class Trainer(DefaultTrainer):\n","    \"\"\"\n","    We use the \"DefaultTrainer\" which contains pre-defined default logic for\n","    standard training workflow. They may not work for you, especially if you\n","    are working on a new research project. In that case you can write your\n","    own training loop. You can use \"tools/plain_train_net.py\" as an example.\n","    \"\"\"\n","\n","    @classmethod\n","    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n","        \"\"\"\n","        Create evaluator(s) for a given dataset.\n","        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n","        For your own dataset, you can simply create an evaluator manually in your\n","        script and do not have to worry about the hacky if-else logic here.\n","        \"\"\"\n","        if output_folder is None:\n","            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n","        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qmn88BmPPyrZ","executionInfo":{"status":"ok","timestamp":1618838643308,"user_tz":240,"elapsed":12216,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"3c2bf6b1-32c0-4519-c444-77776e42cdfd"},"source":["#@title create cfg file and evaluate\n","from detectron2.engine import DefaultTrainer\n","from detectron2.evaluation import COCOEvaluator\n","\n","\n","def defineCFG(cfg, model, max_iter, mnum):\n","  cfg.merge_from_file(model_zoo.get_config_file(model))\n","  cfg.DATASETS.TRAIN = (\"signs_train\",) \n","  cfg.DATASETS.TEST = (\"signs_valid\",)\n","  cfg.TEST.EVAL_PERIOD=500\n","  cfg.DATALOADER.NUM_WORKERS = 2\n","  cfg.MODEL.WEIGHTS = os.path.join(model_path, \"model_final.pth\") \n","  cfg.SOLVER.IMS_PER_BATCH = 2\n","  cfg.SOLVER.BASE_LR = 0.001  # pick a good LR was 0.00025\n","  cfg.SOLVER.MAX_ITER = max_iter    # larger iter for large dataset\n","  cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   \n","  cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(sign_metadata.get(\"thing_classes\")) #gets the number of classes specified for the dataset\n","  cfg.OUTPUT_DIR = model_path\n","\n","cfg = get_cfg()\n","\n","model_num = 5 #@param {type:\"number\"}\n","model_path = getModel(model_num)\n","model = \"PascalVOC-Detection/faster_rcnn_R_50_C4.yaml\" #@param [\"PascalVOC-Detection/faster_rcnn_R_50_C4.yaml\", \"COCO-Detection/faster_rcnn_R_101_C4_3x.yaml\"]\n","max_iter = 26000 #@param {type:\"number\"}\n","defineCFG(cfg, model, max_iter, model_num)\n","trainer = Trainer(cfg)\n","trainer.resume_or_load(resume=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[04/19 13:23:54 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): ResNet(\n","    (stem): BasicStem(\n","      (conv1): Conv2d(\n","        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","      )\n","    )\n","    (res2): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","      )\n","    )\n","    (res3): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","      (3): BottleneckBlock(\n","        (conv1): Conv2d(\n","          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","      )\n","    )\n","    (res4): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (3): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (4): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","      (5): BottleneckBlock(\n","        (conv1): Conv2d(\n","          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): Res5ROIHeads(\n","    (pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (res5): Sequential(\n","      (0): BottleneckBlock(\n","        (shortcut): Conv2d(\n","          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","        (conv1): Conv2d(\n","          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","      )\n","      (1): BottleneckBlock(\n","        (conv1): Conv2d(\n","          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","      )\n","      (2): BottleneckBlock(\n","        (conv1): Conv2d(\n","          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv2): Conv2d(\n","          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","        )\n","        (conv3): Conv2d(\n","          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","        )\n","      )\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=2048, out_features=14, bias=True)\n","      (bbox_pred): Linear(in_features=2048, out_features=52, bias=True)\n","    )\n","  )\n",")\n","\u001b[32m[04/19 13:23:54 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 3252 images left.\n","\u001b[32m[04/19 13:23:54 d2.data.build]: \u001b[0mDistribution of instances among all 13 categories:\n","\u001b[36m|   category    | #instances   |   category   | #instances   |   category   | #instances   |\n","|:-------------:|:-------------|:------------:|:-------------|:------------:|:-------------|\n","| pedestrianC.. | 740          | signalAhead  | 618          | speedLimit35 | 369          |\n","| speedLimit25  | 229          |  keepRight   | 223          |  addedLane   | 207          |\n","|     merge     | 185          |    yield     | 151          |   laneEnds   | 133          |\n","|   stopAhead   | 113          | speedLimit45 | 102          | speedLimit30 | 94           |\n","|    school     | 88           |              |              |              |              |\n","|     total     | 3252         |              |              |              |              |\u001b[0m\n","\u001b[32m[04/19 13:23:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[04/19 13:23:54 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[04/19 13:23:54 d2.data.common]: \u001b[0mSerializing 3252 elements to byte tensors and concatenating them all ...\n","\u001b[32m[04/19 13:23:54 d2.data.common]: \u001b[0mSerialized dataset takes 1.38 MiB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5U_qWMHPRaL","executionInfo":{"status":"ok","timestamp":1618839019529,"user_tz":240,"elapsed":380981,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"a6d22b6d-013d-48b0-843b-ee2f392002f1"},"source":["#@title calculate COCO ap values based on validation set, also get the inference time\n","trainer.test(cfg, trainer.model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[04/19 13:24:02 d2.data.build]: \u001b[0mDistribution of instances among all 13 categories:\n","\u001b[36m|   category    | #instances   |   category   | #instances   |   category   | #instances   |\n","|:-------------:|:-------------|:------------:|:-------------|:------------:|:-------------|\n","| pedestrianC.. | 237          | signalAhead  | 203          | speedLimit35 | 117          |\n","| speedLimit25  | 86           |  keepRight   | 71           |  addedLane   | 58           |\n","|     merge     | 59           |    yield     | 69           |   laneEnds   | 56           |\n","|   stopAhead   | 33           | speedLimit45 | 32           | speedLimit30 | 29           |\n","|    school     | 33           |              |              |              |              |\n","|     total     | 1083         |              |              |              |              |\u001b[0m\n","\u001b[32m[04/19 13:24:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[04/19 13:24:02 d2.data.common]: \u001b[0mSerializing 1083 elements to byte tensors and concatenating them all ...\n","\u001b[32m[04/19 13:24:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/19 13:24:02 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n","\u001b[32m[04/19 13:24:02 d2.evaluation.coco_evaluation]: \u001b[0m'signs_valid' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/19 13:24:02 d2.data.datasets.coco]: \u001b[0mUsing previously cached COCO format annotations at '/content/drive/MyDrive/thesis/COCO/model5/inference/signs_valid_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n","\u001b[32m[04/19 13:24:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 1083 images\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/detectron2/modeling/roi_heads/fast_rcnn.py:154: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  filter_inds = filter_mask.nonzero()\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[04/19 13:24:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/1083. 0.2257 s / img. ETA=0:09:29\n","\u001b[32m[04/19 13:24:29 d2.evaluation.evaluator]: \u001b[0mInference done 18/1083. 0.2258 s / img. ETA=0:11:14\n","\u001b[32m[04/19 13:24:35 d2.evaluation.evaluator]: \u001b[0mInference done 27/1083. 0.2257 s / img. ETA=0:11:10\n","\u001b[32m[04/19 13:24:40 d2.evaluation.evaluator]: \u001b[0mInference done 35/1083. 0.2259 s / img. ETA=0:11:08\n","\u001b[32m[04/19 13:24:45 d2.evaluation.evaluator]: \u001b[0mInference done 51/1083. 0.2261 s / img. ETA=0:09:03\n","\u001b[32m[04/19 13:24:51 d2.evaluation.evaluator]: \u001b[0mInference done 67/1083. 0.2265 s / img. ETA=0:08:02\n","\u001b[32m[04/19 13:24:56 d2.evaluation.evaluator]: \u001b[0mInference done 77/1083. 0.2265 s / img. ETA=0:08:01\n","\u001b[32m[04/19 13:25:01 d2.evaluation.evaluator]: \u001b[0mInference done 91/1083. 0.2265 s / img. ETA=0:07:35\n","\u001b[32m[04/19 13:25:06 d2.evaluation.evaluator]: \u001b[0mInference done 101/1083. 0.2264 s / img. ETA=0:07:39\n","\u001b[32m[04/19 13:25:11 d2.evaluation.evaluator]: \u001b[0mInference done 119/1083. 0.2263 s / img. ETA=0:07:03\n","\u001b[32m[04/19 13:25:16 d2.evaluation.evaluator]: \u001b[0mInference done 136/1083. 0.2263 s / img. ETA=0:06:39\n","\u001b[32m[04/19 13:25:22 d2.evaluation.evaluator]: \u001b[0mInference done 146/1083. 0.2263 s / img. ETA=0:06:41\n","\u001b[32m[04/19 13:25:27 d2.evaluation.evaluator]: \u001b[0mInference done 165/1083. 0.2264 s / img. ETA=0:06:15\n","\u001b[32m[04/19 13:25:32 d2.evaluation.evaluator]: \u001b[0mInference done 180/1083. 0.2264 s / img. ETA=0:06:04\n","\u001b[32m[04/19 13:25:37 d2.evaluation.evaluator]: \u001b[0mInference done 198/1083. 0.2264 s / img. ETA=0:05:47\n","\u001b[32m[04/19 13:25:42 d2.evaluation.evaluator]: \u001b[0mInference done 214/1083. 0.2263 s / img. ETA=0:05:36\n","\u001b[32m[04/19 13:25:47 d2.evaluation.evaluator]: \u001b[0mInference done 231/1083. 0.2263 s / img. ETA=0:05:25\n","\u001b[32m[04/19 13:25:53 d2.evaluation.evaluator]: \u001b[0mInference done 247/1083. 0.2263 s / img. ETA=0:05:17\n","\u001b[32m[04/19 13:25:58 d2.evaluation.evaluator]: \u001b[0mInference done 263/1083. 0.2263 s / img. ETA=0:05:08\n","\u001b[32m[04/19 13:26:03 d2.evaluation.evaluator]: \u001b[0mInference done 278/1083. 0.2263 s / img. ETA=0:05:01\n","\u001b[32m[04/19 13:26:08 d2.evaluation.evaluator]: \u001b[0mInference done 293/1083. 0.2263 s / img. ETA=0:04:54\n","\u001b[32m[04/19 13:26:14 d2.evaluation.evaluator]: \u001b[0mInference done 309/1083. 0.2263 s / img. ETA=0:04:47\n","\u001b[32m[04/19 13:26:19 d2.evaluation.evaluator]: \u001b[0mInference done 324/1083. 0.2262 s / img. ETA=0:04:41\n","\u001b[32m[04/19 13:26:24 d2.evaluation.evaluator]: \u001b[0mInference done 339/1083. 0.2262 s / img. ETA=0:04:34\n","\u001b[32m[04/19 13:26:30 d2.evaluation.evaluator]: \u001b[0mInference done 358/1083. 0.2262 s / img. ETA=0:04:24\n","\u001b[32m[04/19 13:26:35 d2.evaluation.evaluator]: \u001b[0mInference done 372/1083. 0.2262 s / img. ETA=0:04:19\n","\u001b[32m[04/19 13:26:40 d2.evaluation.evaluator]: \u001b[0mInference done 389/1083. 0.2261 s / img. ETA=0:04:11\n","\u001b[32m[04/19 13:26:46 d2.evaluation.evaluator]: \u001b[0mInference done 405/1083. 0.2261 s / img. ETA=0:04:04\n","\u001b[32m[04/19 13:26:51 d2.evaluation.evaluator]: \u001b[0mInference done 422/1083. 0.2261 s / img. ETA=0:03:57\n","\u001b[32m[04/19 13:26:56 d2.evaluation.evaluator]: \u001b[0mInference done 439/1083. 0.2261 s / img. ETA=0:03:49\n","\u001b[32m[04/19 13:27:01 d2.evaluation.evaluator]: \u001b[0mInference done 456/1083. 0.2262 s / img. ETA=0:03:42\n","\u001b[32m[04/19 13:27:06 d2.evaluation.evaluator]: \u001b[0mInference done 473/1083. 0.2262 s / img. ETA=0:03:34\n","\u001b[32m[04/19 13:27:11 d2.evaluation.evaluator]: \u001b[0mInference done 491/1083. 0.2261 s / img. ETA=0:03:27\n","\u001b[32m[04/19 13:27:16 d2.evaluation.evaluator]: \u001b[0mInference done 507/1083. 0.2262 s / img. ETA=0:03:20\n","\u001b[32m[04/19 13:27:21 d2.evaluation.evaluator]: \u001b[0mInference done 526/1083. 0.2262 s / img. ETA=0:03:12\n","\u001b[32m[04/19 13:27:26 d2.evaluation.evaluator]: \u001b[0mInference done 542/1083. 0.2262 s / img. ETA=0:03:06\n","\u001b[32m[04/19 13:27:32 d2.evaluation.evaluator]: \u001b[0mInference done 558/1083. 0.2262 s / img. ETA=0:03:00\n","\u001b[32m[04/19 13:27:37 d2.evaluation.evaluator]: \u001b[0mInference done 576/1083. 0.2262 s / img. ETA=0:02:53\n","\u001b[32m[04/19 13:27:42 d2.evaluation.evaluator]: \u001b[0mInference done 592/1083. 0.2262 s / img. ETA=0:02:47\n","\u001b[32m[04/19 13:27:47 d2.evaluation.evaluator]: \u001b[0mInference done 610/1083. 0.2262 s / img. ETA=0:02:41\n","\u001b[32m[04/19 13:27:52 d2.evaluation.evaluator]: \u001b[0mInference done 627/1083. 0.2262 s / img. ETA=0:02:34\n","\u001b[32m[04/19 13:27:57 d2.evaluation.evaluator]: \u001b[0mInference done 645/1083. 0.2262 s / img. ETA=0:02:27\n","\u001b[32m[04/19 13:28:03 d2.evaluation.evaluator]: \u001b[0mInference done 659/1083. 0.2262 s / img. ETA=0:02:23\n","\u001b[32m[04/19 13:28:08 d2.evaluation.evaluator]: \u001b[0mInference done 677/1083. 0.2262 s / img. ETA=0:02:17\n","\u001b[32m[04/19 13:28:13 d2.evaluation.evaluator]: \u001b[0mInference done 691/1083. 0.2263 s / img. ETA=0:02:12\n","\u001b[32m[04/19 13:28:18 d2.evaluation.evaluator]: \u001b[0mInference done 706/1083. 0.2263 s / img. ETA=0:02:07\n","\u001b[32m[04/19 13:28:23 d2.evaluation.evaluator]: \u001b[0mInference done 725/1083. 0.2263 s / img. ETA=0:02:00\n","\u001b[32m[04/19 13:28:29 d2.evaluation.evaluator]: \u001b[0mInference done 740/1083. 0.2263 s / img. ETA=0:01:55\n","\u001b[32m[04/19 13:28:34 d2.evaluation.evaluator]: \u001b[0mInference done 760/1083. 0.2263 s / img. ETA=0:01:48\n","\u001b[32m[04/19 13:28:40 d2.evaluation.evaluator]: \u001b[0mInference done 776/1083. 0.2263 s / img. ETA=0:01:42\n","\u001b[32m[04/19 13:28:45 d2.evaluation.evaluator]: \u001b[0mInference done 791/1083. 0.2263 s / img. ETA=0:01:37\n","\u001b[32m[04/19 13:28:50 d2.evaluation.evaluator]: \u001b[0mInference done 809/1083. 0.2263 s / img. ETA=0:01:31\n","\u001b[32m[04/19 13:28:55 d2.evaluation.evaluator]: \u001b[0mInference done 826/1083. 0.2262 s / img. ETA=0:01:25\n","\u001b[32m[04/19 13:29:00 d2.evaluation.evaluator]: \u001b[0mInference done 842/1083. 0.2262 s / img. ETA=0:01:20\n","\u001b[32m[04/19 13:29:06 d2.evaluation.evaluator]: \u001b[0mInference done 861/1083. 0.2262 s / img. ETA=0:01:13\n","\u001b[32m[04/19 13:29:11 d2.evaluation.evaluator]: \u001b[0mInference done 876/1083. 0.2262 s / img. ETA=0:01:08\n","\u001b[32m[04/19 13:29:16 d2.evaluation.evaluator]: \u001b[0mInference done 893/1083. 0.2263 s / img. ETA=0:01:03\n","\u001b[32m[04/19 13:29:22 d2.evaluation.evaluator]: \u001b[0mInference done 908/1083. 0.2263 s / img. ETA=0:00:58\n","\u001b[32m[04/19 13:29:27 d2.evaluation.evaluator]: \u001b[0mInference done 925/1083. 0.2263 s / img. ETA=0:00:52\n","\u001b[32m[04/19 13:29:32 d2.evaluation.evaluator]: \u001b[0mInference done 941/1083. 0.2263 s / img. ETA=0:00:47\n","\u001b[32m[04/19 13:29:37 d2.evaluation.evaluator]: \u001b[0mInference done 959/1083. 0.2263 s / img. ETA=0:00:41\n","\u001b[32m[04/19 13:29:42 d2.evaluation.evaluator]: \u001b[0mInference done 973/1083. 0.2263 s / img. ETA=0:00:36\n","\u001b[32m[04/19 13:29:48 d2.evaluation.evaluator]: \u001b[0mInference done 992/1083. 0.2263 s / img. ETA=0:00:30\n","\u001b[32m[04/19 13:29:53 d2.evaluation.evaluator]: \u001b[0mInference done 1009/1083. 0.2263 s / img. ETA=0:00:24\n","\u001b[32m[04/19 13:29:58 d2.evaluation.evaluator]: \u001b[0mInference done 1024/1083. 0.2263 s / img. ETA=0:00:19\n","\u001b[32m[04/19 13:30:03 d2.evaluation.evaluator]: \u001b[0mInference done 1040/1083. 0.2263 s / img. ETA=0:00:14\n","\u001b[32m[04/19 13:30:08 d2.evaluation.evaluator]: \u001b[0mInference done 1056/1083. 0.2263 s / img. ETA=0:00:08\n","\u001b[32m[04/19 13:30:13 d2.evaluation.evaluator]: \u001b[0mInference done 1071/1083. 0.2263 s / img. ETA=0:00:03\n","\u001b[32m[04/19 13:30:17 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:05:55.767994 (0.330026 s / img per device, on 1 devices)\n","\u001b[32m[04/19 13:30:17 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:04:03 (0.226257 s / img per device, on 1 devices)\n","\u001b[32m[04/19 13:30:17 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n","\u001b[32m[04/19 13:30:17 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /content/drive/MyDrive/thesis/COCO/model5/inference/coco_instances_results.json\n","\u001b[32m[04/19 13:30:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n","Loading and preparing results...\n","DONE (t=0.01s)\n","creating index...\n","index created!\n","\u001b[32m[04/19 13:30:18 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n","\u001b[32m[04/19 13:30:18 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.20 seconds.\n","\u001b[32m[04/19 13:30:18 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n","\u001b[32m[04/19 13:30:18 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.05 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.629\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.807\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.760\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.572\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.703\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.765\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.739\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.794\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.794\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.734\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.850\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.901\n","\u001b[32m[04/19 13:30:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n","|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n","|:------:|:------:|:------:|:------:|:------:|:------:|\n","| 62.906 | 80.656 | 75.960 | 57.159 | 70.282 | 76.545 |\n","\u001b[32m[04/19 13:30:18 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n","| category           | AP     | category     | AP     | category     | AP     |\n","|:-------------------|:-------|:-------------|:-------|:-------------|:-------|\n","| pedestrianCrossing | 67.103 | signalAhead  | 58.299 | speedLimit35 | 54.884 |\n","| speedLimit25       | 60.366 | keepRight    | 64.495 | addedLane    | 75.880 |\n","| merge              | 76.598 | yield        | 32.932 | laneEnds     | 55.980 |\n","| stopAhead          | 79.191 | speedLimit45 | 50.256 | speedLimit30 | 74.250 |\n","| school             | 67.545 |              |        |              |        |\n","\u001b[32m[04/19 13:30:18 d2.engine.defaults]: \u001b[0mEvaluation results for signs_valid in csv format:\n","\u001b[32m[04/19 13:30:18 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n","\u001b[32m[04/19 13:30:18 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n","\u001b[32m[04/19 13:30:18 d2.evaluation.testing]: \u001b[0mcopypaste: 62.9061,80.6555,75.9599,57.1590,70.2817,76.5453\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('bbox',\n","              {'AP': 62.90607216671417,\n","               'AP-addedLane': 75.8796424614369,\n","               'AP-keepRight': 64.49494897064935,\n","               'AP-laneEnds': 55.98019302376041,\n","               'AP-merge': 76.59833735157466,\n","               'AP-pedestrianCrossing': 67.10301064080618,\n","               'AP-school': 67.5445285494936,\n","               'AP-signalAhead': 58.299059729097415,\n","               'AP-speedLimit25': 60.36616767623261,\n","               'AP-speedLimit30': 74.24993535738683,\n","               'AP-speedLimit35': 54.88377401218679,\n","               'AP-speedLimit45': 50.25633397427023,\n","               'AP-stopAhead': 79.19083546096066,\n","               'AP-yield': 32.93217095942851,\n","               'AP50': 80.65553947319854,\n","               'AP75': 75.95992274397307,\n","               'APl': 76.54526166902404,\n","               'APm': 70.28168541089194,\n","               'APs': 57.15895812290547})])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"rygdyotWjGYC"},"source":["cfg_file = open(os.path.join(model_path, \"faster-rcnn.yaml\"),'w')\n","cfg_file.write(cfg.dump())\n","cfg_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1lexq72j3AW","executionInfo":{"status":"ok","timestamp":1618839181089,"user_tz":240,"elapsed":9252,"user":{"displayName":"Zachary Newbury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0VbxxJFu_Z5puzJUENjYPLaVS1KvMwrrxh_XsSQ=s64","userId":"08338740760284225846"}},"outputId":"8cc2f0c1-f389-4ed7-89f8-8190f7d17192"},"source":["#@title FPS calculation\n","cfg.MODEL.WEIGHTS = os.path.join(model_path, \"model_final.pth\")  # path to the model we just trained\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25   # set a custom testing threshold\n","cfg.DATASETS.TEST = (\"signs_valid\",)\n","predictor = DefaultPredictor(cfg)\n","\n","im =cv2.imread(\"/content/drive/MyDrive/thesis/LISA dataset/aiua120306-1/frameAnnotations-DataLog02142012_003_external_camera.avi_annotations/pedestrianCrossing_1333396454.avi_image4.png\")\n","\n","import time\n","times = []\n","for i in range(20):\n","    start_time = time.time()\n","    outputs = predictor(im)\n","    delta = time.time() - start_time\n","    times.append(delta)\n","mean_delta = np.array(times).mean()\n","fps = 1 / mean_delta\n","print(\"Average(sec):{:.2f},fps:{:.2f}\".format(mean_delta, fps))\n","\n","#faster r-cnn fps test with /content/drive/MyDrive/thesis/LISA dataset/\n","#aiua120306-1/frameAnnotations-DataLog02142012_003_external_camera.avi\n","#_annotations/pedestrianCrossing_1333396454.avi_image4.png\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Average(sec):0.24,fps:4.10\n"],"name":"stdout"}]}]}